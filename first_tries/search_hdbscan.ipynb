{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c23ed25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ta\n",
    "\n",
    "# to run async code in jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "API_KEY = None\n",
    "SECRET_KEY = None\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if API_KEY is None:\n",
    "    API_KEY = os.environ.get('ALP_API_KEY')\n",
    "\n",
    "if SECRET_KEY is None:\n",
    "    SECRET_KEY = os.environ.get('ALP_SEC_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf5588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "from alpaca.trading.client import TradingClient\n",
    "from alpaca.data.timeframe import TimeFrame, TimeFrameUnit\n",
    "from alpaca.data.historical.corporate_actions import CorporateActionsClient\n",
    "from alpaca.data.historical.stock import StockHistoricalDataClient\n",
    "from alpaca.trading.stream import TradingStream\n",
    "from alpaca.data.live.stock import StockDataStream\n",
    "\n",
    "from alpaca.data.requests import (\n",
    "    CorporateActionsRequest,\n",
    "    StockBarsRequest,\n",
    "    StockQuotesRequest,\n",
    "    StockTradesRequest,\n",
    ")\n",
    "\n",
    "from alpaca.data.enums import Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b31eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regime_tickers import custom_vol_subset, custom_exp_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08241fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_historical_data_client = StockHistoricalDataClient(API_KEY, SECRET_KEY)\n",
    "\n",
    "# alpaca has no older data than 2016-01-04 for this symbol set\n",
    "earliest_date = datetime(2016, 1, 4, tzinfo=ZoneInfo('America/New_York'))\n",
    "# earliest_timestamps_by_symbol = df_adj.reset_index().groupby('symbol')['timestamp'].min()\n",
    "\n",
    "req = StockBarsRequest(\n",
    "    symbol_or_symbols = list(set(custom_vol_subset + custom_exp_subset + ['SPY'])),  # add SPY for market reference\n",
    "    timeframe=TimeFrame(amount = 1, unit = TimeFrameUnit.Day), \n",
    "    start = earliest_date,                    \n",
    "    limit = None,    \n",
    "    adjustment=Adjustment('all') # adjust for splits and dividends                                           \n",
    ")\n",
    "df_adj = stock_historical_data_client.get_stock_bars(req).df.reset_index().set_index('timestamp')\n",
    "df_adj = df_adj.sort_values(by=['symbol', 'timestamp']) # Ensure sorted for correct rolling calcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90cdb3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NANs: 0\n",
      "             symbol\n",
      "open         BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "high         BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "low          BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "close        BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "volume       BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "trade_count  BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "vwap         BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 300)\n",
    "print(\"Total NANs:\", df_adj.pivot(columns=\"symbol\").isna().sum().sum())\n",
    "print(df_adj.pivot(columns=\"symbol\").isna().sum())\n",
    "\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28de4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## windows = [5, 20, 60]\n",
    "windows = [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8281e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Daily Log Returns\n",
    "df_adj['log_returns'] = df_adj.groupby('symbol')['close'].transform(lambda x: np.log(x / x.shift(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b952b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_window = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1aabaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Standard Deviation (Volatility). For further computations, later we will use z-scores of these values only\n",
    "for w in windows:\n",
    "    df_adj[f'rolling_std_20'] = df_adj.groupby('symbol')['log_returns'].transform(\n",
    "        lambda x: x.rolling(window=w, min_periods=w).std()\n",
    "    )\n",
    "\n",
    "    df_adj[f'z_rolling_std_20'] = df_adj.groupby('symbol')[f'rolling_std_20'].transform(\n",
    "        lambda x: (x - x.rolling(z_window).mean()) / x.rolling(z_window).std()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4295d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range Ratio (High - Low) / Close, per asset\n",
    "df_adj['range_ratio'] = (df_adj['high'] - df_adj['low']) / df_adj['close']\n",
    "# Adding smoothing (e.g., rolling average of 5 days)\n",
    "df_adj['range_ratio_smooth'] = df_adj.groupby('symbol')['range_ratio'].transform(lambda x: x.rolling(5).mean())\n",
    "# Standardizing with Z-scores for regime shift detection\n",
    "df_adj['z_range_ratio_smooth'] = df_adj.groupby('symbol')['range_ratio_smooth'].transform(lambda x: (x - x.rolling(z_window).mean()) / x.rolling(z_window).std())\n",
    "\n",
    "df_adj.drop(columns=['range_ratio', 'range_ratio_smooth'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a8dea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility Shock Count\n",
    "# The number of days in a recent lookback window where daily returns exceeded 2 standard deviations, either up or down.\n",
    "# It captures the frequency of abnormal moves â€” a key indicator for risk, panic clusters, or momentum bursts.\n",
    "\n",
    "# 1. Calculate return z-scores per symbol\n",
    "df_adj['z_log_returns'] = df_adj.groupby('symbol')['log_returns'].transform(\n",
    "    lambda x: (x - x.rolling(60).mean()) / x.rolling(60).std()\n",
    ")\n",
    "\n",
    "# 2. Count shocks in rolling window (e.g., abs(z) > 2 over last 20 days)\n",
    "df_adj['vol_shock_count_20'] = df_adj.groupby('symbol')['z_log_returns'].transform(\n",
    "    lambda x: x.rolling(20).apply(lambda r: (abs(r) > 2).sum(), raw=True)\n",
    ")\n",
    "\n",
    "df_adj.drop(columns=['log_returns'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e70601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj = df_adj.sort_values(by=['symbol', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc8ec58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_adj.pivot(columns=\"symbol\")\n",
    "# Flatten columns ('close', 'QQQ') -> 'close_QQQ'\n",
    "df_pivot.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in df_pivot.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c9dac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility Cross-Asset Dispersion lets you measure how much the volatility levels themselves are diverging across assets. It's like saying: are\n",
    "#  all sectors jittery together or are some calm while others are chaotic? Great for reading the marketâ€™s internal stress or divergence.\n",
    "\n",
    "# Step 1: Filter only the 'rolling_std_20' columns\n",
    "rolling_std_cols = [col for col in df_pivot.columns if col.startswith('rolling_std_20_')]\n",
    "\n",
    "# Step 2: Compute cross-sectional std (dispersion) across symbols for each timestamp\n",
    "vol_dispersion = df_pivot[rolling_std_cols].std(axis=1)\n",
    "\n",
    "# Step 3: Add back into df_pivot as a new column\n",
    "df_pivot['volatility_dispersion'] = vol_dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dad288fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realized Vol / Implied Vol compares actual historical price movement (realized vol) with what the market expects (implied vol). \n",
    "# A ratio >1 means realized volatility has exceeded market expectations â€” a sign of surprise, dislocation, or a catch-up in pricing. \n",
    "# A ratio <1 suggests implied vol is elevated â€” often in times of uncertainty or hedging demand.\n",
    "\n",
    "# Z-realized vs Implied Volatility Ratios\n",
    "\n",
    "# Reference realized vol\n",
    "realized = df_pivot['rolling_std_20_IWM']\n",
    "\n",
    "# Loop over each implied asset and compute z-scored realized/implied ratios\n",
    "implied = df_pivot['rolling_std_20_VIXY']\n",
    "ratio = realized / implied\n",
    "z_col = f'z_realized_implied_vixy'\n",
    "\n",
    "df_pivot[z_col] = (ratio - ratio.rolling(z_window).mean()) / ratio.rolling(z_window).std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4f293fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility Spread = VIXY implied vol proxy â€“ realized vol of SPY/IWM\n",
    "# Z-score Level\tWhat It Means\n",
    "# > +2\tImplied vol (VIXY) is much higher than realized vol â†’ Fear spike\n",
    "# < -2\tRealized vol is unusually high vs whatâ€™s priced in â†’ Complacency mispricing?\n",
    "# Around 0\tImplied and realized vol are in sync â†’ Stable regime\n",
    "\n",
    "# Step 1: Compute average realized vol of SPY and IWM\n",
    "realized_avg = df_pivot[['rolling_std_20_SPY', 'rolling_std_20_IWM']].mean(axis=1)\n",
    "\n",
    "# Step 2â€“4: Loop over implied assets to compute z-scored spread and clean up\n",
    "spread = df_pivot['rolling_std_20_VIXY'] - realized_avg\n",
    "z_col = f'z_vol_spread_vixy'\n",
    "\n",
    "df_pivot[z_col] = (spread - spread.rolling(z_window).mean()) / spread.rolling(z_window).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "620f6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility Skew = Difference in realized vol between two symbols\n",
    "# IWM vs SPY â€” Small Caps vs Broad Market - GLD vs SPY â€” Gold vs Broad Market\n",
    "# Define the assets to compare against SPY\n",
    "skew_assets = ['IWM', 'GLD']\n",
    "\n",
    "# Loop to compute z-scored volatility skew vs SPY\n",
    "for ticker in skew_assets:\n",
    "    spread = df_pivot[f'rolling_std_20_{ticker}'] - df_pivot['rolling_std_20_SPY']\n",
    "    z_col = f'z_vol_skew_{ticker.lower()}'\n",
    "    \n",
    "    df_pivot[z_col] = (spread - spread.rolling(z_window).mean()) / spread.rolling(z_window).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab370d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify relevant columns (z-scored rolling std)\n",
    "z_vol_cols = [col for col in df_pivot.columns if col.startswith('z_rolling_std_20_')]\n",
    "\n",
    "# Step 2: Compute rolling correlation matrix and extract average pairwise correlation\n",
    "def compute_volatility_correlation_regime(df, columns, window=20):\n",
    "    avg_corr = []\n",
    "\n",
    "    for i in range(window, len(df)):\n",
    "        window_df = df.iloc[i - window:i][columns]\n",
    "        corr_matrix = window_df.corr()\n",
    "\n",
    "        # Extract upper triangle without diagonal\n",
    "        upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        mean_corr = upper_triangle.stack().mean()\n",
    "        avg_corr.append(mean_corr)\n",
    "\n",
    "    # Align with original index\n",
    "    avg_corr_series = pd.Series(avg_corr, index=df.index[window:])\n",
    "    return avg_corr_series\n",
    "\n",
    "# Step 3: Run the function\n",
    "correlation_regime_vol = compute_volatility_correlation_regime(df_pivot, z_vol_cols, window=20)\n",
    "\n",
    "# Step 4: Add to df_pivot (optional)\n",
    "df_pivot.loc[correlation_regime_vol.index, 'correlation_regime_vol'] = correlation_regime_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9a6334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion vs. Contraction\n",
    "# Advance-Decline Ratio (ADR)\n",
    "\n",
    "# Step 1: Identify close columns\n",
    "close_cols = [col for col in df_pivot.columns if col.startswith('close_')]\n",
    "\n",
    "# Step 2: Calculate previous close using shift\n",
    "prev_close = df_pivot[close_cols].shift(1)\n",
    "\n",
    "# Step 3: Calculate change\n",
    "change = df_pivot[close_cols] - prev_close\n",
    "\n",
    "# Step 4: Classify movement\n",
    "advance = (change > 0).astype(int)\n",
    "decline = (change < 0).astype(int)\n",
    "\n",
    "# Step 5: Count advances and declines per timestamp\n",
    "advancing_count = advance.sum(axis=1)\n",
    "declining_count = decline.sum(axis=1)\n",
    "\n",
    "# Step 6: Calculate ADR  manual encoding to avoid division by zero\n",
    "\n",
    "adr_series = pd.Series(index=df_pivot.index, dtype=float)\n",
    "\n",
    "for i in df_pivot.index:\n",
    "    adv = advancing_count[i]\n",
    "    dec = declining_count[i]\n",
    "    \n",
    "    if adv == 0 and dec == 0: # avoifd division by zero\n",
    "        adr_series[i] = 1.0  # neutral market\n",
    "    elif dec == 0:\n",
    "        adr_series[i] = len(set(custom_vol_subset + custom_exp_subset))  # strong bullish signal\n",
    "    else:\n",
    "        adr_series[i] = adv / dec\n",
    "\n",
    "# analyzing trends, a rolling average helps\n",
    "adr_smoothed = pd.Series(adr_series).rolling(window=5).mean()\n",
    "\n",
    "# Step 7: Add ADR to df_pivot\n",
    "df_pivot['ADR_smooth'] = adr_smoothed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8a02197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion vs. Contraction\n",
    "# Zweig Breadth Thrust (ZBT) is a classic and powerful breadth thrust indicator used to identify the start of strong bullish moves\n",
    "\n",
    "# Step 1: Use previously computed advance matrix\n",
    "# (from your ADR calculation)\n",
    "# advance = (change > 0).astype(int)\n",
    "\n",
    "# Step 2: Compute percentage of advancing stocks per timestamp\n",
    "advancing_percent = advance.sum(axis=1) / advance.shape[1]\n",
    "\n",
    "# Step 3: Compute 10-day moving average of advancing percent, ema more smooth than sma\n",
    "zbt_series = advancing_percent.ewm(span=10, adjust=False).mean()\n",
    "df_pivot['zbt'] = zbt_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b60b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion vs. Contraction\n",
    "# % tickers Above MA50/MA200 Market-wide participation\n",
    "# Step 1: Identify close columns\n",
    "close_cols = [col for col in df_pivot.columns if col.startswith('close_')]\n",
    "\n",
    "# Step 2: Compute MA50 and MA200 for each symbol\n",
    "ma50 = df_pivot[close_cols].rolling(window=50, min_periods=1).mean()\n",
    "ma200 = df_pivot[close_cols].rolling(window=200, min_periods=1).mean()\n",
    "\n",
    "# Step 3: Compare close to MA50 and MA200\n",
    "above_ma50 = (df_pivot[close_cols] > ma50).astype(int)\n",
    "above_ma200 = (df_pivot[close_cols] > ma200).astype(int)\n",
    "\n",
    "# Step 4: Compute % of stocks above each MA\n",
    "pct_above_ma50 = above_ma50.sum(axis=1) / len(close_cols) * 100\n",
    "pct_above_ma200 = above_ma200.sum(axis=1) / len(close_cols) * 100\n",
    "\n",
    "# Step 5: Add to df_pivot\n",
    "df_pivot['Pct_Above_MA50'] = pct_above_ma50\n",
    "df_pivot['Pct_Above_MA200'] = pct_above_ma200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e56a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion vs. Contraction\n",
    "# Number of stocks making a new 20-day high, Number of stocks making a new 20-day low, New Highs vs. New Lows Ratio\n",
    "# Step 1: Identify close columns\n",
    "close_cols = [col for col in df_pivot.columns if col.startswith('close_')]\n",
    "\n",
    "# Step 2: Compute rolling 20-day high and low\n",
    "rolling_high = df_pivot[close_cols].rolling(window=20, min_periods=1).max()\n",
    "rolling_low = df_pivot[close_cols].rolling(window=20, min_periods=1).min()\n",
    "\n",
    "# Step 3: Identify new highs and new lows\n",
    "new_highs = (df_pivot[close_cols] >= rolling_high).astype(int)\n",
    "new_lows = (df_pivot[close_cols] <= rolling_low).astype(int)\n",
    "\n",
    "# Step 4: Count per timestamp\n",
    "nh_count = new_highs.sum(axis=1)\n",
    "nl_count = new_lows.sum(axis=1)\n",
    "\n",
    "# Step 5: Compute NH/NL Ratio with manual encoding to avoid division by zero\n",
    "nh_nl_ratio = pd.Series(index=df_pivot.index, dtype=float)\n",
    "\n",
    "for i in df_pivot.index:\n",
    "    nh = nh_count[i]\n",
    "    nl = nl_count[i]\n",
    "    \n",
    "    if nl == 0 and nh == 0: # avoifd division by zero\n",
    "        nh_nl_ratio[i] = 1.0  # flat market\n",
    "    elif nl == 0: # if there are no new lows, we have a strong bullish signal, avoid division by zero\n",
    "        nh_nl_ratio[i] = len(set(custom_vol_subset + custom_exp_subset))  # strong bullish signal\n",
    "    else:\n",
    "        nh_nl_ratio[i] = nh / nl # normal ratio\n",
    "\n",
    "\n",
    "nh_nl_ratio_smoothed = pd.Series(nh_nl_ratio).rolling(window=5).mean()\n",
    "\n",
    "# Step 6: Add to df_pivot\n",
    "df_pivot['NH_NL_Ratio_smooth'] = nh_nl_ratio_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d617457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion vs. Contraction\n",
    "# Volume Surge feature, which compares total volume at each timestamp to its rolling average\n",
    "# Step 1: Identify volume columns\n",
    "volume_cols = [col for col in df_pivot.columns if col.startswith('volume_')]\n",
    "\n",
    "# Step 2: Compute total volume per timestamp\n",
    "total_volume = df_pivot[volume_cols].sum(axis=1)\n",
    "\n",
    "# Step 3: Compute rolling average volume (e.g., 20-day)\n",
    "rolling_avg_volume = total_volume.rolling(window=20, min_periods=1).mean()\n",
    "\n",
    "# Step 4: Compute volume surge ratio\n",
    "volume_surge_ratio = total_volume / rolling_avg_volume\n",
    "\n",
    "# Step 5: Manual encoding to avoid NaN or inf\n",
    "volume_surge = pd.Series(index=df_pivot.index, dtype=float)\n",
    "\n",
    "for i in df_pivot.index:\n",
    "    tv = total_volume[i]\n",
    "    rv = rolling_avg_volume[i]\n",
    "    \n",
    "    if rv == 0:\n",
    "        volume_surge[i] = 1.0  # neutral if no prior volume\n",
    "    else:\n",
    "        volume_surge[i] = tv / rv\n",
    "\n",
    "df_pivot['volum_surge'] = volume_surge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb14503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'SPY' columns to avoid redundancy, they where just used for comparison\n",
    "df_pivot.drop(columns=[col for col in df_pivot.columns if 'SPY' in col], inplace=True)\n",
    "\n",
    "# Drop 'rolling_std_20' columns to avoid redundancy, they where just used for comparison, we keep z-scores only\n",
    "# Drop columns that start with 'rolling_std_20'\n",
    "df_pivot.drop(columns=[col for col in df_pivot.columns if col.startswith(\"rolling_std_20\")], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7053b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# after 2*max(windows) +z_window +1 days, not nans\n",
    "df_pivot_clean = df_pivot[max(windows)+z_window+1:]\n",
    "\n",
    "print(df_pivot_clean.isna().sum().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe74e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stationary = df_pivot_clean.drop(\n",
    "    columns=[col for col in df_pivot_clean.columns if any(x in col for x in [\"vwap\", \"low\", \"high\", \"close\", \"open\", \"volume\",\n",
    "                                                                              \"trade_count\", \"z_log_returns\", \"z_range_ratio_smooth\", \"z_rolling_std\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd5766bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['vol_shock_count_20_BND', 'vol_shock_count_20_CPER',\n",
       "       'vol_shock_count_20_GLD', 'vol_shock_count_20_HYG',\n",
       "       'vol_shock_count_20_IWM', 'vol_shock_count_20_SCHP',\n",
       "       'vol_shock_count_20_VIXY', 'vol_shock_count_20_XLI',\n",
       "       'volatility_dispersion', 'z_realized_implied_vixy', 'z_vol_spread_vixy',\n",
       "       'z_vol_skew_iwm', 'z_vol_skew_gld', 'correlation_regime_vol',\n",
       "       'ADR_smooth', 'zbt', 'Pct_Above_MA50', 'Pct_Above_MA200',\n",
       "       'NH_NL_Ratio_smooth', 'volum_surge'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stationary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9242463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "921b0906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\Documents\\TFG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=3, n_neig=30, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=5, n_neig=30, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=8, n_neig=30, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Quantile (uniform) | UMAP: n_comp=10, n_neig=30, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=3, n_neig=30, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=5, n_neig=30, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=8, n_neig=30, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: MinMax | UMAP: n_comp=10, n_neig=30, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=3, n_neig=30, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=5, n_neig=30, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=8, n_neig=30, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=8, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=8, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=8, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=8, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=8, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=8, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=15, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=15, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=15, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=15, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=15, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=15, min_d=0.2, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=30, min_d=0.01, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=30, min_d=0.01, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=30, min_d=0.1, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=30, min_d=0.1, metric=euclidean\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=30, min_d=0.2, metric=correlation\n",
      "\n",
      "ðŸ” Scaler: Standard | UMAP: n_comp=10, n_neig=30, min_d=0.2, metric=euclidean\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import (\n",
    "    QuantileTransformer, PowerTransformer, MinMaxScaler,\n",
    "    RobustScaler, MaxAbsScaler, StandardScaler\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import itertools\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "\n",
    "# Define scalers\n",
    "scalers = {\n",
    "    \"Quantile (uniform)\": QuantileTransformer(output_distribution='uniform', random_state=0),\n",
    "    \"MinMax\": MinMaxScaler(),\n",
    "    \"Standard\": StandardScaler(),\n",
    "}\n",
    "\n",
    "# Define parameter grids\n",
    "N_COMP = [3, 5, 8, 10]\n",
    "N_NEIG = [8, 15, 30]\n",
    "MIN_D = [0.01, 0.1, 0.2]\n",
    "METRIC = [\"correlation\", \"euclidean\"]\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = [10, 30]\n",
    "HDBSCAN_MIN_SAMPLES = [15, 20]\n",
    "HDBSCAN_SELECTION_METHOD = ['eom', 'leaf']\n",
    "\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Range of K values to test\n",
    "k_range = range(2, 8)\n",
    "\n",
    "# Loop over scalers\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    scaled = scaler.fit_transform(df_stationary)\n",
    "\n",
    "    # Loop over all UMAP parameter combinations\n",
    "    for n_comp, n_neig, min_d, metric in itertools.product(N_COMP, N_NEIG, MIN_D, METRIC):\n",
    "        print(f\"\\nðŸ” Scaler: {scaler_name} | UMAP: n_comp={n_comp}, n_neig={n_neig}, min_d={min_d}, metric={metric}\")\n",
    "\n",
    "        # Apply UMAP\n",
    "        X_umap = umap.UMAP(\n",
    "            n_neighbors=n_neig,\n",
    "            min_dist=min_d,\n",
    "            n_components=n_comp,\n",
    "            metric=metric\n",
    "        ).fit_transform(scaled)\n",
    "\n",
    "        # Loop over HDBSCAN parameter combinations\n",
    "        for min_cluster_size, min_samples, selection_method in itertools.product(\n",
    "            HDBSCAN_MIN_CLUSTER_SIZE, HDBSCAN_MIN_SAMPLES, HDBSCAN_SELECTION_METHOD\n",
    "        ):\n",
    "            hdb = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples,\n",
    "                metric=metric,\n",
    "                cluster_selection_method=selection_method,\n",
    "            )\n",
    "            hdb_labels = hdb.fit_predict(X_umap)\n",
    "\n",
    "            valid_mask = hdb_labels != -1\n",
    "            if np.sum(valid_mask) > 1:\n",
    "                all_results.append({\n",
    "                    \"scaler\": scaler_name,\n",
    "                    \"method\": \"HDBSCAN\",\n",
    "                    \"k\": len(np.unique(hdb_labels[valid_mask])),\n",
    "                    \"n_components\": n_comp,\n",
    "                    \"n_neighbors\": n_neig,\n",
    "                    \"min_dist\": min_d,\n",
    "                    \"metric\": metric,\n",
    "                    \"min_cluster_size\": min_cluster_size,\n",
    "                    \"min_samples\": min_samples,\n",
    "                    \"selection_method\": selection_method,\n",
    "                    \"bic\": None,\n",
    "                    \"aic\": None,\n",
    "                    \"silhouette\": silhouette_score(X_umap[valid_mask], hdb_labels[valid_mask]),\n",
    "                    \"calinski_harabasz\": calinski_harabasz_score(X_umap[valid_mask], hdb_labels[valid_mask]),\n",
    "                    \"davies_bouldin\": davies_bouldin_score(X_umap[valid_mask], hdb_labels[valid_mask])\n",
    "                })\n",
    "            else:\n",
    "                all_results.append({\n",
    "                    \"scaler\": scaler_name,\n",
    "                    \"method\": \"HDBSCAN\",\n",
    "                    \"k\": 0,\n",
    "                    \"n_components\": n_comp,\n",
    "                    \"n_neighbors\": n_neig,\n",
    "                    \"min_dist\": min_d,\n",
    "                    \"metric\": metric,\n",
    "                    \"min_cluster_size\": min_cluster_size,\n",
    "                    \"min_samples\": min_samples,\n",
    "                    \"selection_method\": selection_method,\n",
    "                    \"bic\": None,\n",
    "                    \"aic\": None,\n",
    "                    \"silhouette\": None,\n",
    "                    \"calinski_harabasz\": None,\n",
    "                    \"davies_bouldin\": None\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05706bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"hdbscan_clustering_gridsearch.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc28213e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaler</th>\n",
       "      <th>method</th>\n",
       "      <th>k</th>\n",
       "      <th>n_components</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>min_dist</th>\n",
       "      <th>metric</th>\n",
       "      <th>min_cluster_size</th>\n",
       "      <th>min_samples</th>\n",
       "      <th>selection_method</th>\n",
       "      <th>bic</th>\n",
       "      <th>aic</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>calinski_harabasz</th>\n",
       "      <th>davies_bouldin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.20</td>\n",
       "      <td>correlation</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.515580</td>\n",
       "      <td>150.478745</td>\n",
       "      <td>0.367808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.496728</td>\n",
       "      <td>647.390991</td>\n",
       "      <td>0.664143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.485729</td>\n",
       "      <td>877.993774</td>\n",
       "      <td>0.621867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.485729</td>\n",
       "      <td>877.993774</td>\n",
       "      <td>0.621867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.485729</td>\n",
       "      <td>877.993774</td>\n",
       "      <td>0.621867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0.20</td>\n",
       "      <td>correlation</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>126.935928</td>\n",
       "      <td>0.402858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.479691</td>\n",
       "      <td>846.375305</td>\n",
       "      <td>0.608116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.477694</td>\n",
       "      <td>839.222046</td>\n",
       "      <td>0.610006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.477694</td>\n",
       "      <td>839.222046</td>\n",
       "      <td>0.610006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.451517</td>\n",
       "      <td>707.771057</td>\n",
       "      <td>0.663327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.451517</td>\n",
       "      <td>707.771057</td>\n",
       "      <td>0.663327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.422767</td>\n",
       "      <td>505.651733</td>\n",
       "      <td>0.637476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.391776</td>\n",
       "      <td>413.546326</td>\n",
       "      <td>0.770996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.390011</td>\n",
       "      <td>549.022644</td>\n",
       "      <td>0.630462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.390011</td>\n",
       "      <td>549.022705</td>\n",
       "      <td>0.630462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>correlation</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.371595</td>\n",
       "      <td>809.621582</td>\n",
       "      <td>0.843506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.362921</td>\n",
       "      <td>457.773865</td>\n",
       "      <td>0.700689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.353180</td>\n",
       "      <td>420.333984</td>\n",
       "      <td>0.657486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.351565</td>\n",
       "      <td>415.398102</td>\n",
       "      <td>0.636382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>MinMax</td>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>eom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.351565</td>\n",
       "      <td>415.398102</td>\n",
       "      <td>0.636382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      scaler   method  k  n_components  n_neighbors  min_dist       metric  \\\n",
       "1138  MinMax  HDBSCAN  2            10           30      0.20  correlation   \n",
       "782   MinMax  HDBSCAN  4             5           15      0.01    euclidean   \n",
       "906   MinMax  HDBSCAN  3             8            8      0.20    euclidean   \n",
       "908   MinMax  HDBSCAN  3             8            8      0.20    euclidean   \n",
       "910   MinMax  HDBSCAN  3             8            8      0.20    euclidean   \n",
       "994   MinMax  HDBSCAN  2             8           30      0.20  correlation   \n",
       "764   MinMax  HDBSCAN  3             5            8      0.20    euclidean   \n",
       "766   MinMax  HDBSCAN  3             5            8      0.20    euclidean   \n",
       "762   MinMax  HDBSCAN  3             5            8      0.20    euclidean   \n",
       "1050  MinMax  HDBSCAN  3            10            8      0.20    euclidean   \n",
       "1054  MinMax  HDBSCAN  3            10            8      0.20    euclidean   \n",
       "1052  MinMax  HDBSCAN  4            10            8      0.20    euclidean   \n",
       "958   MinMax  HDBSCAN  4             8           15      0.20    euclidean   \n",
       "1036  MinMax  HDBSCAN  5            10            8      0.10    euclidean   \n",
       "1038  MinMax  HDBSCAN  5            10            8      0.10    euclidean   \n",
       "806   MinMax  HDBSCAN  8             5           15      0.20  correlation   \n",
       "620   MinMax  HDBSCAN  4             3            8      0.20    euclidean   \n",
       "622   MinMax  HDBSCAN  4             3            8      0.20    euclidean   \n",
       "972   MinMax  HDBSCAN  5             8           30      0.01    euclidean   \n",
       "974   MinMax  HDBSCAN  5             8           30      0.01    euclidean   \n",
       "\n",
       "      min_cluster_size  min_samples selection_method   bic   aic  silhouette  \\\n",
       "1138                10           20              eom  None  None    0.515580   \n",
       "782                 30           20              eom  None  None    0.496728   \n",
       "906                 10           20              eom  None  None    0.485729   \n",
       "908                 30           15              eom  None  None    0.485729   \n",
       "910                 30           20              eom  None  None    0.485729   \n",
       "994                 10           20              eom  None  None    0.482100   \n",
       "764                 30           15              eom  None  None    0.479691   \n",
       "766                 30           20              eom  None  None    0.477694   \n",
       "762                 10           20              eom  None  None    0.477694   \n",
       "1050                10           20              eom  None  None    0.451517   \n",
       "1054                30           20              eom  None  None    0.451517   \n",
       "1052                30           15              eom  None  None    0.422767   \n",
       "958                 30           20              eom  None  None    0.391776   \n",
       "1036                30           15              eom  None  None    0.390011   \n",
       "1038                30           20              eom  None  None    0.390011   \n",
       "806                 30           20              eom  None  None    0.371595   \n",
       "620                 30           15              eom  None  None    0.362921   \n",
       "622                 30           20              eom  None  None    0.353180   \n",
       "972                 30           15              eom  None  None    0.351565   \n",
       "974                 30           20              eom  None  None    0.351565   \n",
       "\n",
       "      calinski_harabasz  davies_bouldin  \n",
       "1138         150.478745        0.367808  \n",
       "782          647.390991        0.664143  \n",
       "906          877.993774        0.621867  \n",
       "908          877.993774        0.621867  \n",
       "910          877.993774        0.621867  \n",
       "994          126.935928        0.402858  \n",
       "764          846.375305        0.608116  \n",
       "766          839.222046        0.610006  \n",
       "762          839.222046        0.610006  \n",
       "1050         707.771057        0.663327  \n",
       "1054         707.771057        0.663327  \n",
       "1052         505.651733        0.637476  \n",
       "958          413.546326        0.770996  \n",
       "1036         549.022644        0.630462  \n",
       "1038         549.022705        0.630462  \n",
       "806          809.621582        0.843506  \n",
       "620          457.773865        0.700689  \n",
       "622          420.333984        0.657486  \n",
       "972          415.398102        0.636382  \n",
       "974          415.398102        0.636382  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df['k']< 10].sort_values(\"silhouette\", ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
