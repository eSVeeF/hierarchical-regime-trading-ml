{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c23ed25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ta\n",
    "\n",
    "# to run async code in jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "API_KEY = None\n",
    "SECRET_KEY = None\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if API_KEY is None:\n",
    "    API_KEY = os.environ.get('ALP_API_KEY')\n",
    "\n",
    "if SECRET_KEY is None:\n",
    "    SECRET_KEY = os.environ.get('ALP_SEC_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acf5588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "from alpaca.trading.client import TradingClient\n",
    "from alpaca.data.timeframe import TimeFrame, TimeFrameUnit\n",
    "from alpaca.data.historical.corporate_actions import CorporateActionsClient\n",
    "from alpaca.data.historical.stock import StockHistoricalDataClient\n",
    "from alpaca.trading.stream import TradingStream\n",
    "from alpaca.data.live.stock import StockDataStream\n",
    "\n",
    "from alpaca.data.requests import (\n",
    "    CorporateActionsRequest,\n",
    "    StockBarsRequest,\n",
    "    StockQuotesRequest,\n",
    "    StockTradesRequest,\n",
    ")\n",
    "\n",
    "from alpaca.data.enums import Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b31eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regime_tickers import custom_vol_subset, custom_exp_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08241fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_historical_data_client = StockHistoricalDataClient(API_KEY, SECRET_KEY)\n",
    "\n",
    "# alpaca has no older data than 2016-01-04 for this symbol set\n",
    "earliest_date = datetime(2016, 1, 4, tzinfo=ZoneInfo('America/New_York'))\n",
    "# earliest_timestamps_by_symbol = df_adj.reset_index().groupby('symbol')['timestamp'].min()\n",
    "\n",
    "req = StockBarsRequest(\n",
    "    symbol_or_symbols = list(set(custom_vol_subset + custom_exp_subset + ['SPY'])),  # add SPY for market reference\n",
    "    timeframe=TimeFrame(amount = 1, unit = TimeFrameUnit.Day), \n",
    "    start = earliest_date,                    \n",
    "    limit = None,    \n",
    "    adjustment=Adjustment('all') # adjust for splits and dividends                                           \n",
    ")\n",
    "df_adj = stock_historical_data_client.get_stock_bars(req).df.reset_index().set_index('timestamp')\n",
    "df_adj = df_adj.sort_values(by=['symbol', 'timestamp']) # Ensure sorted for correct rolling calcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90cdb3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NANs: 0\n",
      "             symbol\n",
      "open         BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "high         BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "low          BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "close        BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "volume       BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "trade_count  BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "vwap         BND       0\n",
      "             CPER      0\n",
      "             GLD       0\n",
      "             HYG       0\n",
      "             IWM       0\n",
      "             SCHP      0\n",
      "             SPY       0\n",
      "             VIXY      0\n",
      "             XLI       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 300)\n",
    "print(\"Total NANs:\", df_adj.pivot(columns=\"symbol\").isna().sum().sum())\n",
    "print(df_adj.pivot(columns=\"symbol\").isna().sum())\n",
    "\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28de4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## windows = [5, 20, 60]\n",
    "windows = [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8281e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Daily Log Returns\n",
    "df_adj['log_returns'] = df_adj.groupby('symbol')['close'].transform(lambda x: np.log(x / x.shift(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b952b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_window = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1aabaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Standard Deviation (Volatility). For further computations, later we will use z-scores of these values only\n",
    "for w in windows:\n",
    "    df_adj[f'rolling_std_20'] = df_adj.groupby('symbol')['log_returns'].transform(\n",
    "        lambda x: x.rolling(window=w, min_periods=w).std()\n",
    "    )\n",
    "\n",
    "    df_adj[f'z_rolling_std_20'] = df_adj.groupby('symbol')[f'rolling_std_20'].transform(\n",
    "        lambda x: (x - x.rolling(z_window).mean()) / x.rolling(z_window).std()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4295d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range Ratio (High - Low) / Close, per asset\n",
    "df_adj['range_ratio'] = (df_adj['high'] - df_adj['low']) / df_adj['close']\n",
    "# Adding smoothing (e.g., rolling average of 5 days)\n",
    "df_adj['range_ratio_smooth'] = df_adj.groupby('symbol')['range_ratio'].transform(lambda x: x.rolling(5).mean())\n",
    "# Standardizing with Z-scores for regime shift detection\n",
    "df_adj['z_range_ratio_smooth'] = df_adj.groupby('symbol')['range_ratio_smooth'].transform(lambda x: (x - x.rolling(z_window).mean()) / x.rolling(z_window).std())\n",
    "\n",
    "df_adj.drop(columns=['range_ratio', 'range_ratio_smooth'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a8dea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility Shock Count\n",
    "# The number of days in a recent lookback window where daily returns exceeded 2 standard deviations, either up or down.\n",
    "# It captures the frequency of abnormal moves — a key indicator for risk, panic clusters, or momentum bursts.\n",
    "\n",
    "# 1. Calculate return z-scores per symbol\n",
    "df_adj['z_log_returns'] = df_adj.groupby('symbol')['log_returns'].transform(\n",
    "    lambda x: (x - x.rolling(60).mean()) / x.rolling(60).std()\n",
    ")\n",
    "\n",
    "# 2. Count shocks in rolling window (e.g., abs(z) > 2 over last 20 days)\n",
    "df_adj['vol_shock_count_20'] = df_adj.groupby('symbol')['z_log_returns'].transform(\n",
    "    lambda x: x.rolling(20).apply(lambda r: (abs(r) > 2).sum(), raw=True)\n",
    ")\n",
    "\n",
    "df_adj.drop(columns=['log_returns'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e70601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj = df_adj.sort_values(by=['symbol', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc8ec58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_adj.pivot(columns=\"symbol\")\n",
    "# Flatten columns ('close', 'QQQ') -> 'close_QQQ'\n",
    "df_pivot.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in df_pivot.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c9dac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility Cross-Asset Dispersion lets you measure how much the volatility levels themselves are diverging across assets. It's like saying: are\n",
    "#  all sectors jittery together or are some calm while others are chaotic? Great for reading the market’s internal stress or divergence.\n",
    "\n",
    "# Step 1: Filter only the 'rolling_std_20' columns\n",
    "rolling_std_cols = [col for col in df_pivot.columns if col.startswith('rolling_std_20_')]\n",
    "\n",
    "# Step 2: Compute cross-sectional std (dispersion) across symbols for each timestamp\n",
    "vol_dispersion = df_pivot[rolling_std_cols].std(axis=1)\n",
    "\n",
    "# Step 3: Add back into df_pivot as a new column\n",
    "df_pivot['volatility_dispersion'] = vol_dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dad288fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realized Vol / Implied Vol compares actual historical price movement (realized vol) with what the market expects (implied vol). \n",
    "# A ratio >1 means realized volatility has exceeded market expectations — a sign of surprise, dislocation, or a catch-up in pricing. \n",
    "# A ratio <1 suggests implied vol is elevated — often in times of uncertainty or hedging demand.\n",
    "\n",
    "# Z-realized vs Implied Volatility Ratios\n",
    "\n",
    "# Reference realized vol\n",
    "realized = df_pivot['rolling_std_20_IWM']\n",
    "\n",
    "# Loop over each implied asset and compute z-scored realized/implied ratios\n",
    "implied = df_pivot['rolling_std_20_VIXY']\n",
    "ratio = realized / implied\n",
    "z_col = f'z_realized_implied_vixy'\n",
    "\n",
    "df_pivot[z_col] = (ratio - ratio.rolling(z_window).mean()) / ratio.rolling(z_window).std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4f293fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility Spread = VIXY implied vol proxy – realized vol of SPY/IWM\n",
    "# Z-score Level\tWhat It Means\n",
    "# > +2\tImplied vol (VIXY) is much higher than realized vol → Fear spike\n",
    "# < -2\tRealized vol is unusually high vs what’s priced in → Complacency mispricing?\n",
    "# Around 0\tImplied and realized vol are in sync → Stable regime\n",
    "\n",
    "# Step 1: Compute average realized vol of SPY and IWM\n",
    "realized_avg = df_pivot[['rolling_std_20_SPY', 'rolling_std_20_IWM']].mean(axis=1)\n",
    "\n",
    "# Step 2–4: Loop over implied assets to compute z-scored spread and clean up\n",
    "spread = df_pivot['rolling_std_20_VIXY'] - realized_avg\n",
    "z_col = f'z_vol_spread_vixy'\n",
    "\n",
    "df_pivot[z_col] = (spread - spread.rolling(z_window).mean()) / spread.rolling(z_window).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "620f6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility Skew = Difference in realized vol between two symbols\n",
    "# QQQ vs SPY — Big Tech vs Broad Market - IWM vs SPY — Small Caps vs Broad Market - GLD vs SPY — Gold vs Broad Market\n",
    "# Define the assets to compare against SPY\n",
    "skew_assets = ['IWM', 'GLD']\n",
    "\n",
    "# Loop to compute z-scored volatility skew vs SPY\n",
    "for ticker in skew_assets:\n",
    "    spread = df_pivot[f'rolling_std_20_{ticker}'] - df_pivot['rolling_std_20_SPY']\n",
    "    z_col = f'z_vol_skew_{ticker.lower()}'\n",
    "    \n",
    "    df_pivot[z_col] = (spread - spread.rolling(z_window).mean()) / spread.rolling(z_window).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9a6334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion vs. Contraction\n",
    "# Advance-Decline Ratio (ADR)\n",
    "\n",
    "# Step 1: Identify close columns\n",
    "close_cols = [col for col in df_pivot.columns if col.startswith('close_')]\n",
    "\n",
    "# Step 2: Calculate previous close using shift\n",
    "prev_close = df_pivot[close_cols].shift(1)\n",
    "\n",
    "# Step 3: Calculate change\n",
    "change = df_pivot[close_cols] - prev_close\n",
    "\n",
    "# Step 4: Classify movement\n",
    "advance = (change > 0).astype(int)\n",
    "decline = (change < 0).astype(int)\n",
    "\n",
    "# Step 5: Count advances and declines per timestamp\n",
    "advancing_count = advance.sum(axis=1)\n",
    "declining_count = decline.sum(axis=1)\n",
    "\n",
    "# Step 6: Calculate ADR  manual encoding to avoid division by zero\n",
    "\n",
    "adr_series = pd.Series(index=df_pivot.index, dtype=float)\n",
    "\n",
    "for i in df_pivot.index:\n",
    "    adv = advancing_count[i]\n",
    "    dec = declining_count[i]\n",
    "    \n",
    "    if adv == 0 and dec == 0: # avoifd division by zero\n",
    "        adr_series[i] = 1.0  # neutral market\n",
    "    elif dec == 0:\n",
    "        adr_series[i] = len(set(custom_vol_subset + custom_exp_subset))  # strong bullish signal\n",
    "    else:\n",
    "        adr_series[i] = adv / dec\n",
    "\n",
    "# analyzing trends, a rolling average helps\n",
    "adr_smoothed = pd.Series(adr_series).rolling(window=5).mean()\n",
    "\n",
    "# Step 7: Add ADR to df_pivot\n",
    "df_pivot['ADR_smooth'] = adr_smoothed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8a02197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion vs. Contraction\n",
    "# Zweig Breadth Thrust (ZBT) is a classic and powerful breadth thrust indicator used to identify the start of strong bullish moves\n",
    "\n",
    "# Step 1: Use previously computed advance matrix\n",
    "# (from your ADR calculation)\n",
    "# advance = (change > 0).astype(int)\n",
    "\n",
    "# Step 2: Compute percentage of advancing stocks per timestamp\n",
    "advancing_percent = advance.sum(axis=1) / advance.shape[1]\n",
    "\n",
    "# Step 3: Compute 10-day moving average of advancing percent, ema more smooth than sma\n",
    "zbt_series = advancing_percent.ewm(span=10, adjust=False).mean()\n",
    "df_pivot['zbt'] = zbt_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b60b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion vs. Contraction\n",
    "# % tickers Above MA50/MA200 Market-wide participation\n",
    "# Step 1: Identify close columns\n",
    "close_cols = [col for col in df_pivot.columns if col.startswith('close_')]\n",
    "\n",
    "# Step 2: Compute MA50 and MA200 for each symbol\n",
    "ma50 = df_pivot[close_cols].rolling(window=50, min_periods=1).mean()\n",
    "ma200 = df_pivot[close_cols].rolling(window=200, min_periods=1).mean()\n",
    "\n",
    "# Step 3: Compare close to MA50 and MA200\n",
    "above_ma50 = (df_pivot[close_cols] > ma50).astype(int)\n",
    "above_ma200 = (df_pivot[close_cols] > ma200).astype(int)\n",
    "\n",
    "# Step 4: Compute % of stocks above each MA\n",
    "pct_above_ma50 = above_ma50.sum(axis=1) / len(close_cols) * 100\n",
    "pct_above_ma200 = above_ma200.sum(axis=1) / len(close_cols) * 100\n",
    "\n",
    "# Step 5: Add to df_pivot\n",
    "df_pivot['Pct_Above_MA50'] = pct_above_ma50\n",
    "df_pivot['Pct_Above_MA200'] = pct_above_ma200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e56a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion vs. Contraction\n",
    "# Number of stocks making a new 20-day high, Number of stocks making a new 20-day low, New Highs vs. New Lows Ratio\n",
    "# Step 1: Identify close columns\n",
    "close_cols = [col for col in df_pivot.columns if col.startswith('close_')]\n",
    "\n",
    "# Step 2: Compute rolling 20-day high and low\n",
    "rolling_high = df_pivot[close_cols].rolling(window=20, min_periods=1).max()\n",
    "rolling_low = df_pivot[close_cols].rolling(window=20, min_periods=1).min()\n",
    "\n",
    "# Step 3: Identify new highs and new lows\n",
    "new_highs = (df_pivot[close_cols] >= rolling_high).astype(int)\n",
    "new_lows = (df_pivot[close_cols] <= rolling_low).astype(int)\n",
    "\n",
    "# Step 4: Count per timestamp\n",
    "nh_count = new_highs.sum(axis=1)\n",
    "nl_count = new_lows.sum(axis=1)\n",
    "\n",
    "# Step 5: Compute NH/NL Ratio with manual encoding to avoid division by zero\n",
    "nh_nl_ratio = pd.Series(index=df_pivot.index, dtype=float)\n",
    "\n",
    "for i in df_pivot.index:\n",
    "    nh = nh_count[i]\n",
    "    nl = nl_count[i]\n",
    "    \n",
    "    if nl == 0 and nh == 0: # avoifd division by zero\n",
    "        nh_nl_ratio[i] = 1.0  # flat market\n",
    "    elif nl == 0: # if there are no new lows, we have a strong bullish signal, avoid division by zero\n",
    "        nh_nl_ratio[i] = len(set(custom_vol_subset + custom_exp_subset))  # strong bullish signal\n",
    "    else:\n",
    "        nh_nl_ratio[i] = nh / nl # normal ratio\n",
    "\n",
    "\n",
    "nh_nl_ratio_smoothed = pd.Series(nh_nl_ratio).rolling(window=5).mean()\n",
    "\n",
    "# Step 6: Add to df_pivot\n",
    "df_pivot['NH_NL_Ratio_smooth'] = nh_nl_ratio_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d617457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion vs. Contraction\n",
    "# Volume Surge feature, which compares total volume at each timestamp to its rolling average\n",
    "# Step 1: Identify volume columns\n",
    "volume_cols = [col for col in df_pivot.columns if col.startswith('volume_')]\n",
    "\n",
    "# Step 2: Compute total volume per timestamp\n",
    "total_volume = df_pivot[volume_cols].sum(axis=1)\n",
    "\n",
    "# Step 3: Compute rolling average volume (e.g., 20-day)\n",
    "rolling_avg_volume = total_volume.rolling(window=20, min_periods=1).mean()\n",
    "\n",
    "# Step 4: Compute volume surge ratio\n",
    "volume_surge_ratio = total_volume / rolling_avg_volume\n",
    "\n",
    "# Step 5: Manual encoding to avoid NaN or inf\n",
    "volume_surge = pd.Series(index=df_pivot.index, dtype=float)\n",
    "\n",
    "for i in df_pivot.index:\n",
    "    tv = total_volume[i]\n",
    "    rv = rolling_avg_volume[i]\n",
    "    \n",
    "    if rv == 0:\n",
    "        volume_surge[i] = 1.0  # neutral if no prior volume\n",
    "    else:\n",
    "        volume_surge[i] = tv / rv\n",
    "\n",
    "df_pivot['volum_surge'] = volume_surge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb14503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'SPY' columns to avoid redundancy, they where just used for comparison\n",
    "df_pivot.drop(columns=[col for col in df_pivot.columns if 'SPY' in col], inplace=True)\n",
    "\n",
    "# Drop 'rolling_std_20' columns to avoid redundancy, they where just used for comparison, we keep z-scores only\n",
    "# Drop columns that start with 'rolling_std_20'\n",
    "df_pivot.drop(columns=[col for col in df_pivot.columns if col.startswith(\"rolling_std_20\")], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7053b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# after 2*max(windows) +z_window -1 days, not nans\n",
    "print(df_pivot[max(windows)+z_window-1:].isna().sum().sum()) \n",
    "\n",
    "df_pivot_clean = df_pivot[max(windows)+z_window-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe74e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stationary = df_pivot_clean.drop(\n",
    "    columns=[col for col in df_pivot_clean.columns if any(x in col for x in [\"vwap\", \"low\", \"high\", \"close\", \"open\", \"volume\",\n",
    "                                                                              \"trade_count\", \"z_log_returns\", \"z_range_ratio_smooth\", \"z_rolling\", \"vol_shock\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd5766bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['volatility_dispersion', 'z_realized_implied_vixy', 'z_vol_spread_vixy',\n",
       "       'z_vol_skew_iwm', 'z_vol_skew_gld', 'ADR_smooth', 'zbt',\n",
       "       'Pct_Above_MA50', 'Pct_Above_MA200', 'NH_NL_Ratio_smooth',\n",
       "       'volum_surge'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stationary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3161ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMP=3\n",
    "N_NEIG=15\n",
    "MIN_D = 0.01\n",
    "METRIC = \"correlation\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ea0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 232\n",
      "1 232\n",
      "2 232\n",
      "3 232\n",
      "4 232\n",
      "5 232\n",
      "6 232\n",
      "7 232\n",
      "8 232\n",
      "9 232\n",
      "10 232\n",
      "11 232\n",
      "12 232\n",
      "13 232\n",
      "14 232\n",
      "15 232\n",
      "16 232\n",
      "17 232\n",
      "18 232\n",
      "19 232\n",
      "20 232\n",
      "21 232\n",
      "22 232\n",
      "23 232\n",
      "24 232\n",
      "25 232\n",
      "26 232\n",
      "27 232\n",
      "28 232\n",
      "29 232\n",
      "30 232\n",
      "31 232\n",
      "32 232\n",
      "33 232\n",
      "34 232\n",
      "35 232\n",
      "36 232\n",
      "37 232\n",
      "38 232\n",
      "39 232\n",
      "40 232\n",
      "41 232\n",
      "42 232\n",
      "43 232\n",
      "44 232\n",
      "45 232\n",
      "46 232\n",
      "47 232\n",
      "48 232\n",
      "49 232\n",
      "50 232\n",
      "51 232\n",
      "52 232\n",
      "53 232\n",
      "54 232\n",
      "55 232\n",
      "56 232\n",
      "57 232\n",
      "58 232\n",
      "59 232\n",
      "60 232\n",
      "61 232\n",
      "62 232\n",
      "63 232\n",
      "64 232\n",
      "65 232\n",
      "66 232\n",
      "67 232\n",
      "68 232\n",
      "69 232\n",
      "70 232\n",
      "71 232\n",
      "72 232\n",
      "73 232\n",
      "74 232\n",
      "75 232\n",
      "76 232\n",
      "77 232\n",
      "78 232\n",
      "79 232\n",
      "80 232\n",
      "81 232\n",
      "82 232\n",
      "83 232\n",
      "84 232\n",
      "85 232\n",
      "86 232\n",
      "87 232\n",
      "88 232\n",
      "89 232\n",
      "90 232\n",
      "91 232\n",
      "92 232\n",
      "93 232\n",
      "94 232\n",
      "95 232\n",
      "96 232\n",
      "97 232\n",
      "98 232\n",
      "99 232\n",
      "100 232\n",
      "101 232\n",
      "102 232\n",
      "103 232\n",
      "104 232\n",
      "105 232\n",
      "106 232\n",
      "107 232\n",
      "108 232\n",
      "109 232\n",
      "110 232\n",
      "111 232\n",
      "112 232\n",
      "113 232\n",
      "114 232\n",
      "115 232\n",
      "116 232\n",
      "117 232\n",
      "118 232\n",
      "119 232\n",
      "120 232\n",
      "121 232\n",
      "122 232\n",
      "123 232\n",
      "124 232\n",
      "125 232\n",
      "126 232\n",
      "127 232\n",
      "128 232\n",
      "129 232\n",
      "130 232\n",
      "131 232\n",
      "132 232\n",
      "133 232\n",
      "134 232\n",
      "135 232\n",
      "136 232\n",
      "137 232\n",
      "138 232\n",
      "139 232\n",
      "140 232\n",
      "141 232\n",
      "142 232\n",
      "143 232\n",
      "144 232\n",
      "145 232\n",
      "146 232\n",
      "147 232\n",
      "148 232\n",
      "149 232\n",
      "150 232\n",
      "151 232\n",
      "152 232\n",
      "153 232\n",
      "154 232\n",
      "155 232\n",
      "156 232\n",
      "157 232\n",
      "158 232\n",
      "159 232\n",
      "160 232\n",
      "161 232\n",
      "162 232\n",
      "163 232\n",
      "164 232\n",
      "165 232\n",
      "166 232\n",
      "167 232\n",
      "168 232\n",
      "169 232\n",
      "170 232\n",
      "171 232\n",
      "172 232\n",
      "173 232\n",
      "174 232\n",
      "175 232\n",
      "176 232\n",
      "177 232\n",
      "178 232\n",
      "179 232\n",
      "180 232\n",
      "181 232\n",
      "182 232\n",
      "183 232\n",
      "184 232\n",
      "185 232\n",
      "186 232\n",
      "187 232\n",
      "188 232\n",
      "189 232\n",
      "190 232\n",
      "191 232\n",
      "192 232\n",
      "193 232\n",
      "194 232\n",
      "195 232\n",
      "196 232\n",
      "197 232\n",
      "198 232\n",
      "199 232\n",
      "200 232\n",
      "201 232\n",
      "202 232\n",
      "203 232\n",
      "204 232\n",
      "205 232\n",
      "206 232\n",
      "207 232\n",
      "208 232\n",
      "209 232\n",
      "210 232\n",
      "211 232\n",
      "212 232\n",
      "213 232\n",
      "214 232\n",
      "215 232\n",
      "216 232\n",
      "217 232\n",
      "218 232\n",
      "219 232\n",
      "220 232\n",
      "221 232\n",
      "222 232\n",
      "223 232\n",
      "224 232\n",
      "225 232\n",
      "226 232\n",
      "227 232\n",
      "228 232\n",
      "229 232\n",
      "230 232\n",
      "231 232\n",
      "232 232\n",
      "233 232\n",
      "234 232\n",
      "235 232\n",
      "236 232\n",
      "237 232\n",
      "238 232\n",
      "239 232\n",
      "240 232\n",
      "241 232\n",
      "242 232\n",
      "243 232\n",
      "244 232\n",
      "245 232\n",
      "246 232\n",
      "247 232\n",
      "248 232\n",
      "249 232\n",
      "250 232\n",
      "251 232\n",
      "252 232\n",
      "253 232\n",
      "254 232\n",
      "255 232\n",
      "256 232\n",
      "257 232\n",
      "258 232\n",
      "259 232\n",
      "260 232\n",
      "261 232\n",
      "262 232\n",
      "263 232\n",
      "264 232\n",
      "265 232\n",
      "266 232\n",
      "267 232\n",
      "268 232\n",
      "269 232\n",
      "270 232\n",
      "271 232\n",
      "272 232\n",
      "273 232\n",
      "274 232\n",
      "275 232\n",
      "276 232\n",
      "277 232\n",
      "278 232\n",
      "279 232\n",
      "280 232\n",
      "281 232\n",
      "282 232\n",
      "283 232\n",
      "284 232\n",
      "285 232\n",
      "286 232\n",
      "287 232\n",
      "288 232\n",
      "289 232\n",
      "290 232\n",
      "291 232\n",
      "292 232\n",
      "293 232\n",
      "294 232\n",
      "295 232\n",
      "296 232\n",
      "297 232\n",
      "298 232\n",
      "299 232\n",
      "300 232\n",
      "301 232\n",
      "302 232\n",
      "303 232\n",
      "304 232\n",
      "305 232\n",
      "306 232\n",
      "307 232\n",
      "308 232\n",
      "309 232\n",
      "310 232\n",
      "311 232\n",
      "312 232\n",
      "313 232\n",
      "314 232\n",
      "315 232\n",
      "316 232\n",
      "317 232\n",
      "318 232\n",
      "319 232\n",
      "320 232\n",
      "321 232\n",
      "322 232\n",
      "323 232\n",
      "324 232\n",
      "325 232\n",
      "326 232\n",
      "327 232\n",
      "328 232\n",
      "329 232\n",
      "330 232\n",
      "331 232\n",
      "332 232\n",
      "333 232\n",
      "334 232\n",
      "335 232\n",
      "336 232\n",
      "337 232\n",
      "338 232\n",
      "339 232\n",
      "340 232\n",
      "341 232\n",
      "342 232\n",
      "343 232\n",
      "344 232\n",
      "345 232\n",
      "346 232\n",
      "347 232\n",
      "348 232\n",
      "349 232\n",
      "350 232\n",
      "351 232\n",
      "352 232\n",
      "353 232\n",
      "354 232\n",
      "355 232\n",
      "356 232\n",
      "357 232\n",
      "358 232\n",
      "359 232\n",
      "360 232\n",
      "361 232\n",
      "362 232\n",
      "363 232\n",
      "364 232\n",
      "365 232\n",
      "366 232\n",
      "367 232\n",
      "368 232\n",
      "369 232\n",
      "370 232\n",
      "371 232\n",
      "372 232\n",
      "373 232\n",
      "374 232\n",
      "375 232\n",
      "376 232\n",
      "377 232\n",
      "378 232\n",
      "379 232\n",
      "380 232\n",
      "381 232\n",
      "382 232\n",
      "383 232\n",
      "384 232\n",
      "385 232\n",
      "386 232\n",
      "387 232\n",
      "388 232\n",
      "389 232\n",
      "390 232\n",
      "391 232\n",
      "392 232\n",
      "393 232\n",
      "394 232\n",
      "395 232\n",
      "396 232\n",
      "397 232\n",
      "398 232\n",
      "399 232\n",
      "400 232\n",
      "401 232\n",
      "402 232\n",
      "403 232\n",
      "404 232\n",
      "405 232\n",
      "406 232\n",
      "407 232\n",
      "408 232\n",
      "409 232\n",
      "410 232\n",
      "411 232\n",
      "412 232\n",
      "413 232\n",
      "414 232\n",
      "415 232\n",
      "416 232\n",
      "417 232\n",
      "418 232\n",
      "419 232\n",
      "420 232\n",
      "421 232\n",
      "422 232\n",
      "423 232\n",
      "424 232\n",
      "425 232\n",
      "426 232\n",
      "427 232\n",
      "428 232\n",
      "429 232\n",
      "430 232\n",
      "431 232\n",
      "432 232\n",
      "433 232\n",
      "434 232\n",
      "435 232\n",
      "436 232\n",
      "437 232\n",
      "438 232\n",
      "439 232\n",
      "440 232\n",
      "441 232\n",
      "442 232\n",
      "443 232\n",
      "444 232\n",
      "445 232\n",
      "446 232\n",
      "447 232\n",
      "448 232\n",
      "449 232\n",
      "450 232\n",
      "451 232\n",
      "452 232\n",
      "453 232\n",
      "454 232\n",
      "455 232\n",
      "456 232\n",
      "457 232\n",
      "458 232\n",
      "459 232\n",
      "460 232\n",
      "461 232\n",
      "462 232\n",
      "463 232\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define column list\n",
    "columns = [\n",
    "    'volatility_dispersion', 'z_realized_implied_vixy', 'z_vol_spread_vixy',\n",
    "    'z_vol_skew_iwm', 'z_vol_skew_gld', 'ADR_smooth', 'zbt',\n",
    "    'Pct_Above_MA50', 'Pct_Above_MA200', 'NH_NL_Ratio_smooth',\n",
    "    'volum_surge'\n",
    "]\n",
    "\n",
    "# Generate all subsets of length 8 to 11\n",
    "column_subsets = [list(subset) for r in range(8, len(columns)+1) for subset in combinations(columns, r)]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "counter = 0\n",
    "\n",
    "for subset in column_subsets:\n",
    "    df_subset = df_stationary[subset]\n",
    "    scaled = MinMaxScaler().fit_transform(df_subset)\n",
    "    X_umap = umap.UMAP(n_neighbors=N_NEIG, min_dist=MIN_D, n_components=N_COMP, metric=METRIC).fit_transform(scaled)\n",
    "\n",
    "    for k in [3, 4]:\n",
    "        gmm = GaussianMixture(n_components=k, random_state=0)\n",
    "        gmm_labels = gmm.fit_predict(X_umap)\n",
    "\n",
    "        result = {\n",
    "            \"subset\": subset,\n",
    "            \"k\": k,\n",
    "            \"silhouette\": silhouette_score(X_umap, gmm_labels),\n",
    "            \"calinski_harabasz\": calinski_harabasz_score(X_umap, gmm_labels),\n",
    "            \"davies_bouldin\": davies_bouldin_score(X_umap, gmm_labels)\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(counter, len(column_subsets))\n",
    "        counter+=1\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df_sorted = results_df.sort_values(by=\"silhouette\", ascending=False)\n",
    "results_df_sorted.to_csv(\"gmm_subset_cols.csv\", index=False)\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: sort by silhouette score\n",
    "results_df_sorted = results_df.sort_values(by=\"silhouette\", ascending=False)\n",
    "results_df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import umap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Range of K values to test\n",
    "k = 3\n",
    "\n",
    "# Scale and apply UMAP\n",
    "scaled = MinMaxScaler().fit_transform(df_stationary)\n",
    "X_umap = umap.UMAP(n_neighbors=N_NEIG, min_dist=MIN_D, n_components=N_COMP, metric=METRIC).fit_transform(scaled)\n",
    "\n",
    "silhouette_scores = []\n",
    "ch_scores = []\n",
    "db_scores = []\n",
    "\n",
    "gmm = GaussianMixture(n_components=k, random_state=0)\n",
    "gmm_labels = gmm.fit_predict(X_umap)\n",
    "\n",
    "print(\"gmm\", {\n",
    "    \"silhouette\": silhouette_score(X_umap, gmm_labels),\n",
    "    \"calinski_harabasz\": calinski_harabasz_score(X_umap, gmm_labels),\n",
    "    \"davies_bouldin\": davies_bouldin_score(X_umap, gmm_labels)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9710057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define component pairs for plotting\n",
    "component_pairs = [(0, 1), (0, 2), (1, 2)]\n",
    "titles = [\n",
    "    \"UMAP Component 1 vs 2\",\n",
    "    \"UMAP Component 1 vs 3\",\n",
    "    \"UMAP Component 2 vs 3\"\n",
    "]\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "def plot_gmm_std_ellipses(ax, gmm, comp_x, comp_y, colors):\n",
    "    from scipy.stats import chi2\n",
    "\n",
    "    std_levels = [1, 2, 3]\n",
    "    chi2_vals = [chi2.ppf(0.3935, df=2), chi2.ppf(0.8647, df=2), chi2.ppf(0.9889, df=2)]\n",
    "    scales = [np.sqrt(val) for val in chi2_vals]\n",
    "\n",
    "    for i, (mean, covar) in enumerate(zip(gmm.means_, gmm.covariances_)):\n",
    "        if covar.ndim == 1:  # spherical\n",
    "            cov = np.diag(covar)\n",
    "        elif covar.ndim == 2:  # full or tied\n",
    "            cov = covar\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Project to 2D\n",
    "        sub_cov = cov[np.ix_([comp_x, comp_y], [comp_x, comp_y])]\n",
    "        sub_mean = mean[[comp_x, comp_y]]\n",
    "\n",
    "        # Eigen decomposition\n",
    "        vals, vecs = np.linalg.eigh(sub_cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        vals, vecs = vals[order], vecs[:, order]\n",
    "        theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "\n",
    "        for scale, std in zip(scales, std_levels):\n",
    "            width, height = 2 * scale * np.sqrt(vals)\n",
    "\n",
    "            if std == 1:\n",
    "                linestyle = '-'\n",
    "            elif std == 2:\n",
    "                linestyle = '--'\n",
    "            else:\n",
    "                linestyle = ':'\n",
    "\n",
    "            ellipse = Ellipse(\n",
    "                sub_mean, width, height, angle=theta,\n",
    "                edgecolor=colors[i], facecolor='none', lw=1.5, linestyle=linestyle\n",
    "            )\n",
    "            ax.add_patch(ellipse)\n",
    "\n",
    "\n",
    "# Define colors for ellipses\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, k))\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (comp_x, comp_y) in enumerate(component_pairs):\n",
    "    ax = axes[i]\n",
    "    scatter = ax.scatter(\n",
    "        X_umap[:, comp_x], X_umap[:, comp_y],\n",
    "        c=gmm_labels, cmap='tab10', s=40, alpha=0.7\n",
    "    )\n",
    "    ax.set_xlabel(f'UMAP Component {comp_x + 1}')\n",
    "    ax.set_ylabel(f'UMAP Component {comp_y + 1}')\n",
    "    ax.set_title(titles[i])\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Plot GMM ellipses\n",
    "    plot_gmm_std_ellipses(ax, gmm, comp_x, comp_y, colors)\n",
    "\n",
    "# Add legend to the last plot\n",
    "from matplotlib.lines import Line2D\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color='black', lw=1.5, linestyle='-'),\n",
    "    Line2D([0], [0], color='black', lw=1.5, linestyle='--'),\n",
    "    Line2D([0], [0], color='black', lw=1.5, linestyle=':')\n",
    "]\n",
    "axes[-1].legend(custom_lines, ['1σ (~39%)', '2σ (~86%)', '3σ (~99%)'], title=\"Confidence Ellipses\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8441ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### creo que es mejor entrenar un modelo para cada combinacion de clusters. kmeans con Wasserstein distance nos da hard labels, mas simple,\n",
    "# GMM nos da soft labels (probs), mas complejo, pero podria apañar algo como threholds the probs, es decir un modelo para cuando la probabilidad de \n",
    "# cluster 1 es mayor que 0.5, otro para cuando es mayor que 0.7, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Explained Variance Ratio ---\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(f\"Total variance explained by {X_pca.shape[1]} components: {pca.explained_variance_ratio_.sum():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b52d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUSCAR UN GRUPO QUE NO TENGA UNA CORRELACION ALTA?????\n",
    "\n",
    "# # # ##### Correlation Heatmap #####\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Optionally drop rows with missing values to ensure clean correlation\n",
    "corr_df = df_stationary.dropna()\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = corr_df.corr()\n",
    "\n",
    "# Plot the correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Close Prices by Symbol\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for symbol in ['UVXY', 'VIXY']:\n",
    "    subset = df_adj[df_adj['symbol'] == symbol]\n",
    "    ax.plot(subset.index, subset['log_returns'], label=symbol)\n",
    "\n",
    "ax.set_title('Close Prices: PDBC vs DBC')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Close Price')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
